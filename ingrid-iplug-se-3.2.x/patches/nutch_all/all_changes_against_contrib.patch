Only in ./: .classpath
Only in ./: .gitignore
Only in ./: .project
Only in ./: .settings
diff -crBE ./build.xml ../local/build.xml
*** ./build.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/build.xml	2010-03-12 11:24:02.000000000 +0100
***************
*** 33,38 ****
--- 33,41 ----
    	<fileset dir="${lib.dir}/admin-ui-ext">
    		<include name="*.jar" />
    	</fileset>  	
+   	<fileset dir="${lib.dir}/ingrid-ext">
+   		<include name="*.jar" />
+   	</fileset>  	
    </path>
  
    <!-- the unit test classpath -->
***************
*** 101,107 ****
      <javac 
       encoding="${build.encoding}" 
       srcdir="${src.dir}"
!      includes="org/apache/nutch/**/*.java"
       destdir="${build.classes}"
       debug="${javac.debug}"
       optimize="${javac.optimize}"
--- 104,110 ----
      <javac 
       encoding="${build.encoding}" 
       srcdir="${src.dir}"
!      includes="**/*.java"
       destdir="${build.classes}"
       debug="${javac.debug}"
       optimize="${javac.optimize}"
***************
*** 139,147 ****
      <copy file="${conf.dir}/nutch-site.xml"
            todir="${build.classes}"/>
      <jar jarfile="${build.dir}/${final.name}.jar"
!          basedir="${build.classes}">
!       <manifest>
!       </manifest>
      </jar>
    </target>
  
--- 142,148 ----
      <copy file="${conf.dir}/nutch-site.xml"
            todir="${build.classes}"/>
      <jar jarfile="${build.dir}/${final.name}.jar"
!          basedir="${build.classes}" manifest="../MANIFEST.MF">
      </jar>
    </target>
  
***************
*** 531,542 ****
    <!-- ================================================================== -->
    <!--                                                                    -->
    <!-- ================================================================== -->
!   <target name="package" depends="jar, job, war, javadoc">
      <mkdir dir="${dist.dir}"/>
      <mkdir dir="${dist.dir}/lib"/>
      <mkdir dir="${dist.dir}/bin"/>
-     <mkdir dir="${dist.dir}/docs"/>
-     <mkdir dir="${dist.dir}/docs/api"/>
      <mkdir dir="${dist.dir}/plugins"/>
  
      <copy todir="${dist.dir}/lib" includeEmptyDirs="false">
--- 532,541 ----
    <!-- ================================================================== -->
    <!--                                                                    -->
    <!-- ================================================================== -->
!   <target name="package" depends="jar, job">
      <mkdir dir="${dist.dir}"/>
      <mkdir dir="${dist.dir}/lib"/>
      <mkdir dir="${dist.dir}/bin"/>
      <mkdir dir="${dist.dir}/plugins"/>
  
      <copy todir="${dist.dir}/lib" includeEmptyDirs="false">
***************
*** 553,559 ****
  
      <copy file="${build.dir}/${final.name}.jar" todir="${dist.dir}"/>
      <copy file="${build.dir}/${final.name}.job" todir="${dist.dir}"/>
-     <copy file="${build.dir}/${final.name}.war" todir="${dist.dir}"/>
  
      <copy todir="${dist.dir}/bin">
        <fileset dir="bin"/>
--- 552,557 ----
***************
*** 567,580 ****
          <fileset dir="${dist.dir}/bin"/>
      </chmod>
  
-     <copy todir="${dist.dir}/docs">
-       <fileset dir="${docs.dir}"/>
-     </copy>
- 
-     <copy todir="${dist.dir}/docs/api">
-       <fileset dir="${build.javadoc}"/>
-     </copy>
- 
      <copy todir="${dist.dir}">
        <fileset dir=".">
          <include name="*.txt" />
--- 565,570 ----
***************
*** 582,593 ****
        </fileset>
      </copy>
  
-     <copy todir="${dist.dir}/src" includeEmptyDirs="true">
-       <fileset dir="src"/>
-     </copy>
- 
      <copy todir="${dist.dir}/" file="build.xml"/>
      <copy todir="${dist.dir}/" file="default.properties"/>
  
    </target>
  
--- 572,588 ----
        </fileset>
      </copy>
  
      <copy todir="${dist.dir}/" file="build.xml"/>
      <copy todir="${dist.dir}/" file="default.properties"/>
+   	<copy todir="${dist.dir}/" file="start-search.sh"/>
+   	<copy todir="${dist.dir}/" file="start-index.sh"/>
+   	<copy todir="${dist.dir}/" >
+ 		<fileset dir=".">
+ 			<include name="*.sh" />
+ 		</fileset>
+ 	</copy>
+   	<chmod file="${dist.dir}/bin/*" perm="754"/>
+   	<chmod file="${dist.dir}/start-*.sh" perm="754"/>
  
    </target>
  
diff -crBE ./conf/nutch-default.xml ../local/conf/nutch-default.xml
*** ./conf/nutch-default.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/conf/nutch-default.xml	2010-03-12 11:24:29.000000000 +0100
***************
*** 1300,1303 ****
--- 1300,1309 ----
    <description>space delimited list of IPreCrawl Implementations to execute these classes in order</description>
  </property>
  
+ <property>
+     <name>scheduling.create.crawl</name>
+     <value>false</value>
+     <description>If true, scheduling will create each time a new crawl. Otherwise the last created crawl will be continued. If now crawls exist by now, the scheduler will create one.</description>
+ </property>
+ 
  </configuration>
diff -crBE ./src/java/org/apache/nutch/crawl/CrawlTool.java ../local/src/java/org/apache/nutch/crawl/CrawlTool.java
*** ./src/java/org/apache/nutch/crawl/CrawlTool.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/crawl/CrawlTool.java	2010-03-12 11:24:21.000000000 +0100
***************
*** 10,15 ****
--- 10,16 ----
  import org.apache.hadoop.fs.FileStatus;
  import org.apache.hadoop.fs.FileSystem;
  import org.apache.hadoop.fs.Path;
+ import org.apache.nutch.admin.searcher.SearcherFactory;
  import org.apache.nutch.crawl.bw.BWInjector;
  import org.apache.nutch.crawl.bw.BWUpdateDb;
  import org.apache.nutch.crawl.metadata.MetadataInjector;
***************
*** 18,28 ****
--- 19,33 ----
  import org.apache.nutch.indexer.DeleteDuplicates;
  import org.apache.nutch.indexer.IndexMerger;
  import org.apache.nutch.indexer.Indexer;
+ import org.apache.nutch.mail.MailService;
  import org.apache.nutch.parse.ParseSegment;
  import org.apache.nutch.segment.SegmentMerger;
  import org.apache.nutch.tools.HostStatistic;
+ import org.apache.nutch.tools.UrlReporter;
  import org.apache.nutch.util.HadoopFSUtil;
  
+ import de.ingrid.iplug.se.SearchUpdateScanner;
+ 
  public class CrawlTool {
  
    private final Configuration _configuration;
***************
*** 75,80 ****
--- 80,89 ----
      // other jobs
      Generator generator = new Generator(_configuration);
      Fetcher fetcher = new Fetcher(_configuration);
+     // ------ none nutch-specific code starts here
+     UrlReporter reporter = new UrlReporter(_configuration);
+     MailService mail = MailService.get(_configuration);
+     // ------ none nutch-specific code ends here
      ParseSegment parseSegment = new ParseSegment(_configuration);
      BWUpdateDb bwUpdateDb = new BWUpdateDb(_configuration);
      ParseDataUpdater parseDataUpdater = new ParseDataUpdater(_configuration);
***************
*** 128,133 ****
--- 137,146 ----
        segs.add(segment);
        fetcher.fetch(segment, threads, org.apache.nutch.fetcher.Fetcher
                .isParsing(_configuration)); // fetch it
+       // ------ none nutch-specific code starts here
+       reporter.analyze(segment);
+       mail.sendSegmentReport(_fileSystem, segment, i);
+       // ------ none nutch-specific code ends here
        if (!Fetcher.isParsing(_configuration)) {
          parseSegment.parse(segment); // parse it, if needed
        }
***************
*** 147,157 ****
      Path[] mergeSegments = HadoopFSUtil.getPaths(listStatus);
      // list of all segments that will be deleted after indexing
      Path[] segmentsToDelete = null;
!     if (i > 0) {
        try {
          // merge segments
          SegmentMerger segmentMerger = new SegmentMerger(_configuration);
          Path mergeDir = new Path(segments, "merge-segments");
          segmentMerger.merge(mergeDir, mergeSegments, false, false, 0);
          // get merged segment
          Path mergeSegTemp = _fileSystem.listStatus(mergeDir)[0].getPath();
--- 160,173 ----
      Path[] mergeSegments = HadoopFSUtil.getPaths(listStatus);
      // list of all segments that will be deleted after indexing
      Path[] segmentsToDelete = null;
!     if (mergeSegments.length > 1) {
        try {
          // merge segments
          SegmentMerger segmentMerger = new SegmentMerger(_configuration);
          Path mergeDir = new Path(segments, "merge-segments");
+         if (_fileSystem.exists(mergeDir)) {
+             _fileSystem.delete(mergeDir, true);
+         }
          segmentMerger.merge(mergeDir, mergeSegments, false, false, 0);
          // get merged segment
          Path mergeSegTemp = _fileSystem.listStatus(mergeDir)[0].getPath();
***************
*** 165,171 ****
          segmentsToDelete = mergeSegments;
          mergeSegments = new Path[] { mergeSegment };
        } catch (Exception e) {
!         e.printStackTrace();
        }
      }
      
--- 181,187 ----
          segmentsToDelete = mergeSegments;
          mergeSegments = new Path[] { mergeSegment };
        } catch (Exception e) {
!         LOG.warn("error while merging" ,e);
        }
      }
      
***************
*** 207,212 ****
--- 223,231 ----
        }
      }
      
+     SearchUpdateScanner.updateCrawl(_fileSystem, _crawlDir);
+     SearcherFactory.getInstance(_configuration).reload();
+     
      if (LOG.isInfoEnabled()) {
        LOG.info("crawl finished: " + _crawlDir);
      }
diff -crBE ./src/java/org/apache/nutch/crawl/NutchWritable.java ../local/src/java/org/apache/nutch/crawl/NutchWritable.java
*** ./src/java/org/apache/nutch/crawl/NutchWritable.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/crawl/NutchWritable.java	2010-03-12 11:23:31.000000000 +0100
***************
*** 18,23 ****
--- 18,24 ----
  
  import org.apache.hadoop.io.Writable;
  import org.apache.nutch.util.GenericWritableConfigurable;
+ import de.ingrid.iplug.se.crawl.sns.CompressedSnsData;
  
  public class NutchWritable extends GenericWritableConfigurable {
    
***************
*** 47,53 ****
        org.apache.nutch.protocol.ProtocolStatus.class,
        org.apache.nutch.searcher.Hit.class,
        org.apache.nutch.searcher.HitDetails.class,
!       org.apache.nutch.searcher.Hits.class
      };
    }
  
--- 48,57 ----
        org.apache.nutch.protocol.ProtocolStatus.class,
        org.apache.nutch.searcher.Hit.class,
        org.apache.nutch.searcher.HitDetails.class,
!       org.apache.nutch.searcher.Hits.class,
!       /// TODO rwe: none nutch specific code start:
!       CompressedSnsData.class
!       /// none nutch specific code end. 
      };
    }
  
diff -crBE ./src/java/org/apache/nutch/fetcher/Fetcher.java ../local/src/java/org/apache/nutch/fetcher/Fetcher.java
*** ./src/java/org/apache/nutch/fetcher/Fetcher.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/fetcher/Fetcher.java	2010-03-12 11:23:55.000000000 +0100
***************
*** 47,52 ****
--- 47,55 ----
  import org.apache.nutch.scoring.ScoringFilters;
  import org.apache.nutch.util.*;
  
+ import de.ingrid.iplug.se.communication.InterplugInCommunicationConstants;
+ import de.ingrid.iplug.se.communication.InterplugInQueueCommunication;
+ import org.apache.nutch.parse.resulthandler.SnsParseResultHandler;
  
  /** 
   * A queue-based fetcher.
***************
*** 109,114 ****
--- 112,119 ----
      }
    }
  
+   private SnsParseResultHandler _snsParseResultHandler;
+   
    private OutputCollector<Text, NutchWritable> output;
    private Reporter reporter;
    
***************
*** 534,539 ****
--- 539,555 ----
                }
                ProtocolOutput output = protocol.getProtocolOutput(fit.url, fit.datum);
                ProtocolStatus status = output.getStatus();
+               
+               // ------ none nutch-specific code starts here
+               // Add protocoll's status and the url into InterplugInQueueCommunication object. A 
+               // consumer in another thread will update the database with this information.
+               InterplugInQueueCommunication<String> instanceForQueues = InterplugInQueueCommunication
+                   .getInstanceForStringQueues();
+               instanceForQueues
+                   .offer(InterplugInCommunicationConstants.URLSTATUS_KEY, status.getCode() + ":" + fit.url);
+               LOG.debug("Set status code '" + status.toString() + "' for url '" + fit.url + "'.");
+               // ------ none nutch-specific code ends here
+               
                Content content = output.getContent();
                ParseStatus pstatus = null;
                // unblock queue
***************
*** 773,778 ****
--- 789,796 ----
                SignatureFactory.getSignature(getConf()).calculate(content, 
                    new ParseStatus().getEmptyParse(conf));
              datum.setSignature(signature);
+           } else {
+             _snsParseResultHandler.process(content, parseResult);
            }
          }
          
***************
*** 903,909 ****
      // set non-blocking & no-robots mode for HTTP protocol plugins.
      getConf().setBoolean(Protocol.CHECK_BLOCKING, false);
      getConf().setBoolean(Protocol.CHECK_ROBOTS, false);
!     
      for (int i = 0; i < threadCount; i++) {       // spawn threads
        new FetcherThread(getConf()).start();
      }
--- 921,933 ----
      // set non-blocking & no-robots mode for HTTP protocol plugins.
      getConf().setBoolean(Protocol.CHECK_BLOCKING, false);
      getConf().setBoolean(Protocol.CHECK_ROBOTS, false);
! 
!     if (_snsParseResultHandler == null) {
!       _snsParseResultHandler = new SnsParseResultHandler();
!     }
!     // start the sns analyzing
!     _snsParseResultHandler.beginParsing(getConf().get(Nutch.SEGMENT_NAME_KEY), (JobConf) getConf());
! 
      for (int i = 0; i < threadCount; i++) {       // spawn threads
        new FetcherThread(getConf()).start();
      }
***************
*** 911,916 ****
--- 935,941 ----
      // select a timeout that avoids a task timeout
      long timeout = getConf().getInt("mapred.task.timeout", 10*60*1000)/2;
  
+     try {
      do {                                          // wait for threads to exit
        try {
          Thread.sleep(1000);
***************
*** 934,939 ****
--- 959,968 ----
      } while (activeThreads.get() > 0);
      LOG.info("-activeThreads=" + activeThreads);
      
+     } finally {
+       // stop the sns analyzing
+       _snsParseResultHandler.stopParsing(getConf().get(Nutch.SEGMENT_NAME_KEY));
+     }
    }
  
    public void fetch(Path segment, int threads, boolean parsing)
diff -crBE ./src/java/org/apache/nutch/indexer/IndexerMapReduce.java ../local/src/java/org/apache/nutch/indexer/IndexerMapReduce.java
*** ./src/java/org/apache/nutch/indexer/IndexerMapReduce.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/indexer/IndexerMapReduce.java	2010-03-12 11:24:06.000000000 +0100
***************
*** 23,28 ****
--- 23,29 ----
  import org.apache.commons.logging.Log;
  import org.apache.commons.logging.LogFactory;
  import org.apache.hadoop.conf.Configured;
+ import org.apache.hadoop.fs.FileSystem;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.io.Writable;
***************
*** 47,52 ****
--- 48,56 ----
  import org.apache.nutch.scoring.ScoringFilterException;
  import org.apache.nutch.scoring.ScoringFilters;
  
+ import de.ingrid.iplug.se.crawl.sns.CompressedSnsData;
+ import de.ingrid.iplug.se.crawl.sns.SnsParseImpl;
+ 
  public class IndexerMapReduce extends Configured
  implements Mapper<Text, Writable, Text, NutchWritable>,
            Reducer<Text, NutchWritable, Text, NutchDocument> {
***************
*** 75,80 ****
--- 79,87 ----
      CrawlDatum fetchDatum = null;
      ParseData parseData = null;
      ParseText parseText = null;
+     /// TODO rwe: none nutch specific code start:
+     CompressedSnsData snsData = null;
+     /// none nutch specific code end. 
      while (values.hasNext()) {
        final Writable value = values.next().get(); // unwrap
        if (value instanceof Inlinks) {
***************
*** 97,102 ****
--- 104,111 ----
          parseData = (ParseData)value;
        } else if (value instanceof ParseText) {
          parseText = (ParseText)value;
+       } else if (value instanceof CompressedSnsData) {
+         snsData = (CompressedSnsData)value;
        } else if (LOG.isWarnEnabled()) {
          LOG.warn("Unrecognized type: "+value.getClass());
        }
***************
*** 121,127 ****
      // add digest, used by dedup
      doc.add("digest", metadata.get(Nutch.SIGNATURE_KEY));
  
!     final Parse parse = new ParseImpl(parseText, parseData);
      try {
        // extract information from dbDatum and pass it to
        // fetchDatum so that indexing filters can use it
--- 130,139 ----
      // add digest, used by dedup
      doc.add("digest", metadata.get(Nutch.SIGNATURE_KEY));
  
! //    final Parse parse = new ParseImpl(parseText, parseData);
!     /// TODO rwe: none nutch specific code start:
!     final Parse parse = ((snsData != null)? new SnsParseImpl(parseText, parseData, snsData) : new ParseImpl(parseText, parseData));
!     /// none nutch specific code end. 
      try {
        // extract information from dbDatum and pass it to
        // fetchDatum so that indexing filters can use it
***************
*** 173,178 ****
--- 185,198 ----
        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.PARSE_DIR_NAME));
        FileInputFormat.addInputPath(job, new Path(segment, ParseData.DIR_NAME));
        FileInputFormat.addInputPath(job, new Path(segment, ParseText.DIR_NAME));
+       /// TODO rwe: none nutch specific code start: 
+       Path path = new Path(segment, CompressedSnsData.DIR_NAME);
+       try {
+         if (FileSystem.get(job).exists(path)) {
+           FileInputFormat.addInputPath(job, path);
+         }
+       } catch (IOException e) { }
+       /// none nutch specific code end. 
      }
  
      FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));
Only in ../local/src/java/org/apache/nutch: mail
Only in ../local/src/java/org/apache/nutch/parse: resulthandler
diff -crBE ./src/java/org/apache/nutch/searcher/FetchedSegments.java ../local/src/java/org/apache/nutch/searcher/FetchedSegments.java
*** ./src/java/org/apache/nutch/searcher/FetchedSegments.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/searcher/FetchedSegments.java	2010-03-12 11:23:23.000000000 +0100
***************
*** 242,250 ****
    public Summary getSummary(HitDetails details, Query query)
      throws IOException {
  
-     if (this.summarizer == null) { return new Summary(); }
- 
      final Segment segment = getSegment(details);
      final ParseText parseText = segment.getParseText(getUrl(details));
      final String text = (parseText != null) ? parseText.getText() : "";
  
--- 242,250 ----
    public Summary getSummary(HitDetails details, Query query)
      throws IOException {
  
      final Segment segment = getSegment(details);
+     if (this.summarizer == null || segment == null) { return new Summary(); }
+ 
      final ParseText parseText = segment.getParseText(getUrl(details));
      final String text = (parseText != null) ? parseText.getText() : "";
  
diff -crBE ./src/java/org/apache/nutch/searcher/Hits.java ../local/src/java/org/apache/nutch/searcher/Hits.java
*** ./src/java/org/apache/nutch/searcher/Hits.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/searcher/Hits.java	2010-03-12 11:24:21.000000000 +0100
***************
*** 32,38 ****
    private boolean totalIsExact = true;
    private Hit[] top;
  
!   public Hits() {}
  
    public Hits(long total, Hit[] top) {
      this.total = total;
--- 32,40 ----
    private boolean totalIsExact = true;
    private Hit[] top;
  
!   public Hits() {
!     top = new Hit[0];
!   }
  
    public Hits(long total, Hit[] top) {
      this.total = total;
diff -crBE ./src/java/org/apache/nutch/searcher/Query.java ../local/src/java/org/apache/nutch/searcher/Query.java
*** ./src/java/org/apache/nutch/searcher/Query.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/searcher/Query.java	2010-03-12 11:23:29.000000000 +0100
***************
*** 24,29 ****
--- 24,31 ----
  import java.io.InputStreamReader;
  import java.util.Arrays;
  import java.util.ArrayList;
+ import java.util.LinkedList;
+ import java.util.List;
  
  // Commons Logging imports
  import org.apache.commons.logging.Log;
***************
*** 34,39 ****
--- 36,42 ----
  import org.apache.hadoop.io.Writable;
  import org.apache.nutch.analysis.AnalyzerFactory;
  
+ import org.apache.nutch.searcher.Query.Clause.NutchClause;
  import org.apache.nutch.analysis.NutchAnalysis;
  import org.apache.nutch.util.NutchConfiguration;
  
***************
*** 57,62 ****
--- 60,104 ----
  
      private Configuration conf; 
  
+     public static class NutchClause {
+       private boolean _required;
+       private boolean _prohibited;
+       private List<Clause> _clauses;
+       private List<NutchClause> _nutchClauses;
+ 
+       public NutchClause(boolean required, boolean prohibited) {
+         _required = required;
+         _prohibited = prohibited;
+         _clauses = new LinkedList<Clause>();
+         _nutchClauses = new LinkedList<NutchClause>();
+       }
+ 
+       public void addClause(Clause clause) {
+         _clauses.add(clause);
+       }
+ 
+       public Clause[] getClauses() {
+         return (Clause[]) _clauses.toArray(new Clause[_clauses.size()]);
+       }
+ 
+       public void addNutchClause(NutchClause clause) {
+         _nutchClauses.add(clause);
+       }
+ 
+       public NutchClause[] getNutchClauses() {
+         return (NutchClause[]) _nutchClauses
+             .toArray(new NutchClause[_nutchClauses.size()]);
+       }
+ 
+       public boolean isRequired() {
+         return _required;
+       }
+ 
+       public boolean isProhibited() {
+         return _prohibited;
+       }
+     }
+ 
      public Clause(Term term, String field,
                    boolean isRequired, boolean isProhibited, Configuration conf) {
        this(term, isRequired, isProhibited, conf);
***************
*** 284,289 ****
--- 326,333 ----
  
    private ArrayList<Clause> clauses = new ArrayList<Clause>();
  
+   private ArrayList<NutchClause> _nutchClauses = new ArrayList<NutchClause>();
+ 
    private Configuration conf;
  
    private static final Clause[] CLAUSES_PROTO = new Clause[0];
***************
*** 318,323 ****
--- 362,377 ----
      clauses.add(new Clause(new Term(term), field, true, false, this.conf));
    }
  
+   /** Add a non required term in the default field. */
+   public void addNonRequiredTerm(String term) {
+     addNonRequiredTerm(term, Clause.DEFAULT_FIELD);
+   }
+ 
+   /** Add a non required term in a specified field. */
+   public void addNonRequiredTerm(String term, String field) {
+     clauses.add(new Clause(new Term(term), field, false, false, this.conf));
+   }
+ 
    /** Add a prohibited term in the default field. */
    public void addProhibitedTerm(String term) {
      addProhibitedTerm(term, Clause.DEFAULT_FIELD);
***************
*** 343,348 ****
--- 397,418 ----
      }
    }
  
+   /** Add a non required phrase in the default field. */
+   public void addNonRequiredPhrase(String[] terms) {
+     addNonRequiredPhrase(terms, Clause.DEFAULT_FIELD);
+   }
+ 
+   /** Add a non required phrase in the specified field. */
+   public void addNonRequiredPhrase(String[] terms, String field) {
+     if (terms.length == 0) { // ignore empty phrase
+     } else if (terms.length == 1) {
+       addNonRequiredTerm(terms[0], field); // optimize to term query
+     } else {
+       clauses
+           .add(new Clause(new Phrase(terms), field, false, false, this.conf));
+     }
+   }
+ 
    /** Add a prohibited phrase in the default field. */
    public void addProhibitedPhrase(String[] terms) {
      addProhibitedPhrase(terms, Clause.DEFAULT_FIELD);
***************
*** 358,363 ****
--- 428,442 ----
      }
    }
  
+   public void addNutchClause(NutchClause clause) {
+     _nutchClauses.add(clause);
+   }
+ 
+   public NutchClause[] getNutchClauses() {
+     return (NutchClause[]) _nutchClauses.toArray(new NutchClause[_nutchClauses
+         .size()]);
+   }
+ 
    public void write(DataOutput out) throws IOException {
      out.writeByte(clauses.size());
      for (int i = 0; i < clauses.size(); i++)
diff -crBE ./src/java/org/apache/nutch/segment/SegmentMerger.java ../local/src/java/org/apache/nutch/segment/SegmentMerger.java
*** ./src/java/org/apache/nutch/segment/SegmentMerger.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/java/org/apache/nutch/segment/SegmentMerger.java	2010-03-12 11:24:29.000000000 +0100
***************
*** 63,68 ****
--- 63,70 ----
  import org.apache.nutch.util.NutchConfiguration;
  import org.apache.nutch.util.NutchJob;
  
+ import de.ingrid.iplug.se.crawl.sns.CompressedSnsData;
+ 
  /**
   * This tool takes several segments and merges their data together. Only the
   * latest versions of data is retained.
***************
*** 198,203 ****
--- 200,206 ----
          MapFile.Writer pt_out = null;
          SequenceFile.Writer g_out = null;
          SequenceFile.Writer p_out = null;
+         SequenceFile.Writer sns_out = null;
          HashMap sliceWriters = new HashMap();
          String segmentName = job.get("segment.merger.segmentName");
          
***************
*** 234,259 ****
            } else if (o instanceof ParseText) {
              pt_out = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);
              pt_out.append(key, o);
            }
          }
          
          // lazily create SequenceFile-s.
          private SequenceFile.Writer ensureSequenceFile(String slice, String dirName) throws IOException {
!           if (slice == null) slice = DEFAULT_SLICE;
!           SequenceFile.Writer res = (SequenceFile.Writer)sliceWriters.get(slice + dirName);
!           if (res != null) return res;
!           Path wname;
!           Path out = FileOutputFormat.getOutputPath(job);
!           if (slice == DEFAULT_SLICE) {
!             wname = new Path(new Path(new Path(out, segmentName), dirName), name);
!           } else {
!             wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);
!           }
!           res = SequenceFile.createWriter(fs, job, wname, Text.class, 
!               CrawlDatum.class,
!               SequenceFileOutputFormat.getOutputCompressionType(job), progress);
!           sliceWriters.put(slice + dirName, res);
!           return res;
          }
  
          // lazily create MapFile-s.
--- 237,268 ----
            } else if (o instanceof ParseText) {
              pt_out = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);
              pt_out.append(key, o);
+           } else if (o instanceof CompressedSnsData) {
+             sns_out = ensureSequenceFile(slice, CompressedSnsData.DIR_NAME, CompressedSnsData.class);
+             sns_out.append(key, o);
            }
          }
          
          // lazily create SequenceFile-s.
          private SequenceFile.Writer ensureSequenceFile(String slice, String dirName) throws IOException {
!           return ensureSequenceFile(slice, dirName, CrawlDatum.class);
!         }
!         
!         private SequenceFile.Writer ensureSequenceFile(String slice, String dirName, Class<? extends Writable> clazz) throws IOException {
!             if (slice == null) slice = DEFAULT_SLICE;
!             SequenceFile.Writer res = (SequenceFile.Writer)sliceWriters.get(slice + dirName);
!             if (res != null) return res;
!             Path wname;
!             Path out = FileOutputFormat.getOutputPath(job);
!             if (slice == DEFAULT_SLICE) {
!               wname = new Path(new Path(new Path(out, segmentName), dirName), name);
!             } else {
!               wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);
!             }
!             res = SequenceFile.createWriter(fs, job, wname, Text.class, clazz,
!                 SequenceFileOutputFormat.getOutputCompressionType(job), progress);
!             sliceWriters.put(slice + dirName, res);
!             return res;
          }
  
          // lazily create MapFile-s.
***************
*** 365,376 ****
--- 374,387 ----
      Content lastC = null;
      ParseData lastPD = null;
      ParseText lastPT = null;
+     CompressedSnsData lastSNS = null;
      String lastGname = null;
      String lastFname = null;
      String lastSigname = null;
      String lastCname = null;
      String lastPDname = null;
      String lastPTname = null;
+     String lastSNSname = null;
      TreeMap<String, ArrayList<CrawlDatum>> linked =
        new TreeMap<String, ArrayList<CrawlDatum>>();
      while (values.hasNext()) {
***************
*** 460,465 ****
--- 471,486 ----
              lastPTname = sp.segmentName;
            }
          }
+       } else if (o instanceof CompressedSnsData) {
+         if (lastSNS == null) {
+           lastSNS = (CompressedSnsData)o;
+           lastSNSname = sp.segmentName;
+         } else {
+           if (lastSNSname.compareTo(sp.segmentName) < 0) {
+             lastSNS = (CompressedSnsData)o;
+             lastSNSname = sp.segmentName;
+           }
+         }
        }
      }
      curCount++;
***************
*** 513,518 ****
--- 534,546 ----
        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());
        output.collect(key, wrapper);
      }
+     if (lastSNS != null) {
+       wrapper.set(lastSNS);
+       sp.partName = CompressedSnsData.DIR_NAME;
+       sp.segmentName = lastSNSname;
+       wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());
+       output.collect(key, wrapper);
+     }
      if (linked.size() > 0) {
        String name = linked.lastKey();
        sp.partName = CrawlDatum.PARSE_DIR_NAME;
***************
*** 546,551 ****
--- 574,580 ----
      boolean c = true;
      boolean pd = true;
      boolean pt = true;
+     boolean sns = true;
      for (int i = 0; i < segs.length; i++) {
        if (!fs.exists(segs[i])) {
          if (LOG.isWarnEnabled()) {
***************
*** 563,574 ****
--- 592,605 ----
        Path pDir = new Path(segs[i], CrawlDatum.PARSE_DIR_NAME);
        Path pdDir = new Path(segs[i], ParseData.DIR_NAME);
        Path ptDir = new Path(segs[i], ParseText.DIR_NAME);
+       Path snsDir = new Path(segs[i], CompressedSnsData.DIR_NAME);
        c = c && fs.exists(cDir);
        g = g && fs.exists(gDir);
        f = f && fs.exists(fDir);
        p = p && fs.exists(pDir);
        pd = pd && fs.exists(pdDir);
        pt = pt && fs.exists(ptDir);
+       sns = sns && fs.exists(snsDir);
      }
      StringBuffer sb = new StringBuffer();
      if (c) sb.append(" " + Content.DIR_NAME);
***************
*** 577,582 ****
--- 608,614 ----
      if (p) sb.append(" " + CrawlDatum.PARSE_DIR_NAME);
      if (pd) sb.append(" " + ParseData.DIR_NAME);
      if (pt) sb.append(" " + ParseText.DIR_NAME);
+     if (sns) sb.append(" " + CompressedSnsData.DIR_NAME);
      if (LOG.isInfoEnabled()) {
        LOG.info("SegmentMerger: using segment data from:" + sb.toString());
      }
***************
*** 606,611 ****
--- 638,647 ----
          Path ptDir = new Path(segs[i], ParseText.DIR_NAME);
          FileInputFormat.addInputPath(job, ptDir);
        }
+       if (sns) {
+         Path snsDir = new Path (segs[i], CompressedSnsData.DIR_NAME);
+         FileInputFormat.addInputPath(job, snsDir);
+       }
      }
      job.setInputFormat(ObjectInputFormat.class);
      job.setMapperClass(SegmentMerger.class);
Only in ../local/src/java/org/apache/nutch/tools: UrlReporter.java
Only in ../local/src/java/org/apache/nutch/util: ZipUtil.java
diff -crBE ./src/plugin/admin-configuration/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-configuration/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-configuration/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-configuration/src/webapp/WEB-INF/web.xml	2010-03-12 11:23:48.000000000 +0100
***************
*** 55,61 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 55,64 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-crawl/src/java/org/apache/nutch/admin/crawl/StartCrawlRunnable.java ../local/src/plugin/admin-crawl/src/java/org/apache/nutch/admin/crawl/StartCrawlRunnable.java
*** ./src/plugin/admin-crawl/src/java/org/apache/nutch/admin/crawl/StartCrawlRunnable.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-crawl/src/java/org/apache/nutch/admin/crawl/StartCrawlRunnable.java	2010-03-12 11:24:42.000000000 +0100
***************
*** 43,59 ****
      FileSystem fileSystem = _crawlTool.getFileSystem();
      Path crawlDir = _crawlTool.getCrawlDir();
      Path lockPath = new Path(crawlDir, "crawl.running");
      try {
!       fileSystem.createNewFile(lockPath);
!       _crawlTool.preCrawl();
!       _crawlTool.crawl(_topN, _depth);
      } catch (IOException e) {
        LOG.warn("can not start crawl.", e);
      } finally {
!       try {
!         fileSystem.delete(lockPath, false);
!       } catch (Throwable e) {
!         LOG.warn("Can not delete lock file.", e);
        }
      }
    }
--- 43,67 ----
      FileSystem fileSystem = _crawlTool.getFileSystem();
      Path crawlDir = _crawlTool.getCrawlDir();
      Path lockPath = new Path(crawlDir, "crawl.running");
+     boolean alreadyRunning = false;
      try {
!       alreadyRunning = fileSystem.exists(lockPath);
!       if (!alreadyRunning) {
!         fileSystem.createNewFile(lockPath);
!         _crawlTool.preCrawl();
!         _crawlTool.crawl(_topN, _depth);
!       } else {
!           LOG.warn("crawl is already running");
!       }
      } catch (IOException e) {
        LOG.warn("can not start crawl.", e);
      } finally {
!       if (!alreadyRunning) {
!         try {
!           fileSystem.delete(lockPath, false);
!         } catch (Throwable e) {
!           LOG.warn("Can not delete lock file.", e);
!         }
        }
      }
    }
diff -crBE ./src/plugin/admin-crawl/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-crawl/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-crawl/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-crawl/src/webapp/WEB-INF/web.xml	2010-03-12 11:24:07.000000000 +0100
***************
*** 63,69 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 63,72 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-instance/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-instance/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-instance/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-instance/src/webapp/WEB-INF/web.xml	2010-03-12 11:24:12.000000000 +0100
***************
*** 60,66 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 60,69 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingController.java ../local/src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingController.java
*** ./src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingController.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingController.java	2010-03-12 11:23:37.000000000 +0100
***************
*** 85,91 ****
  
    @ModelAttribute("depths")
    public Integer[] depth() {
!     Integer[] month = new Integer[10];
      for (int i = 0; i < month.length; i++) {
        month[i] = i;
      }
--- 85,91 ----
  
    @ModelAttribute("depths")
    public Integer[] depth() {
!     Integer[] month = new Integer[11];
      for (int i = 0; i < month.length; i++) {
        month[i] = i;
      }
diff -crBE ./src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingRunnable.java ../local/src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingRunnable.java
*** ./src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingRunnable.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-scheduling/src/java/org/apache/nutch/admin/scheduling/SchedulingRunnable.java	2010-03-12 11:23:37.000000000 +0100
***************
*** 25,32 ****
--- 25,34 ----
  import org.apache.commons.logging.Log;
  import org.apache.commons.logging.LogFactory;
  import org.apache.hadoop.conf.Configuration;
+ import org.apache.hadoop.fs.FileStatus;
  import org.apache.hadoop.fs.FileSystem;
  import org.apache.hadoop.fs.Path;
+ import org.apache.hadoop.fs.PathFilter;
  import org.apache.nutch.admin.ConfigurationUtil;
  import org.apache.nutch.crawl.CrawlTool;
  
***************
*** 63,68 ****
--- 65,71 ----
        LOCK = true;
        FileSystem fileSystem = null;
        Path lockPath = null;
+       boolean alreadyRunning = false;
        try {
  
          File crawlDirectory = crawlData.getWorkingDirectory();
***************
*** 73,97 ****
          Configuration configuration = configurationUtil
                  .loadConfiguration(crawlDirectory.getName());
          fileSystem = FileSystem.get(configuration);
!         String folderName = "Crawl-" + _format.format(new Date());
          Path crawlDir = new Path(path, folderName);
          fileSystem.mkdirs(crawlDir);
  
          lockPath = new Path(crawlDir, "crawl.running");
!         fileSystem.createNewFile(lockPath);
!         CrawlTool crawlTool = new CrawlTool(configuration, crawlDir);
!         crawlTool.preCrawl();
!         crawlTool.crawl(crawlData.getTopn(), crawlData.getDepth());
        } catch (Throwable e) {
          LOG.error("crawl fails.", e);
        } finally {
-         LOG.info("unlock the scheduled crawl: "
-                 + crawlData.getWorkingDirectory().getAbsolutePath());
          LOCK = false;
!         try {
!           fileSystem.delete(lockPath, false);
!         } catch (IOException e) {
!           LOG.warn("can not delete lock file.", e);
          }
        }
      } else {
--- 76,107 ----
          Configuration configuration = configurationUtil
                  .loadConfiguration(crawlDirectory.getName());
          fileSystem = FileSystem.get(configuration);
!         String folderName = createFolderName(configuration, fileSystem, path);
          Path crawlDir = new Path(path, folderName);
          fileSystem.mkdirs(crawlDir);
  
          lockPath = new Path(crawlDir, "crawl.running");
!         alreadyRunning = fileSystem.exists(lockPath);
!         if (!alreadyRunning) {
!           fileSystem.createNewFile(lockPath);
!           CrawlTool crawlTool = new CrawlTool(configuration, crawlDir);
!           crawlTool.preCrawl();
!           crawlTool.crawl(crawlData.getTopn(), crawlData.getDepth());
!         } else {
!             LOG.warn("crawl is already running");
!         }
        } catch (Throwable e) {
          LOG.error("crawl fails.", e);
        } finally {
          LOCK = false;
!         if (!alreadyRunning) {
!           LOG.info("unlock the scheduled crawl: "
!                   + crawlData.getWorkingDirectory().getAbsolutePath());
!           try {
!             fileSystem.delete(lockPath, false);
!           } catch (IOException e) {
!             LOG.warn("can not delete lock file.", e);
!           }
          }
        }
      } else {
***************
*** 101,105 ****
--- 111,140 ----
      }
  
    }
+   
+   private String createFolderName(final Configuration conf, final FileSystem fs, final Path crawls) throws IOException {
+       if (!conf.getBoolean("scheduling.create.crawl", true)) {
+          final FileStatus[] files = fs.listStatus(crawls, new PathFilter() {
+             public boolean accept(Path p) {
+                 try {
+                     return p.getName().startsWith("Crawl-") && !fs.isFile(p);
+                 } catch (IOException e) {
+                     return false;
+                 }
+             }
+          });
+          if (files != null && files.length > 0) {
+              String last = "";
+              for(final FileStatus file : files) {
+                  final String name = file.getPath().getName();
+                  if (name.compareTo(last) > 0) {
+                      last = name;
+                  }
+              }
+              return last;
+          }
+       }
+       return "Crawl-" + _format.format(new Date());
+   }
  
  }
diff -crBE ./src/plugin/admin-scheduling/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-scheduling/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-scheduling/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-scheduling/src/webapp/WEB-INF/web.xml	2010-03-12 11:23:24.000000000 +0100
***************
*** 61,67 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 61,70 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-search/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-search/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-search/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-search/src/webapp/WEB-INF/web.xml	2010-03-12 11:23:51.000000000 +0100
***************
*** 55,61 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 55,64 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-system/src/webapp/WEB-INF/jsp/system.jsp ../local/src/plugin/admin-system/src/webapp/WEB-INF/jsp/system.jsp
*** ./src/plugin/admin-system/src/webapp/WEB-INF/jsp/system.jsp	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-system/src/webapp/WEB-INF/jsp/system.jsp	2010-03-12 11:23:47.000000000 +0100
***************
*** 99,113 ****
  						var startImage = document.getElementById('start');
  						var stopImage = document.getElementById('stop');
  						if(action == 'start'){
! 							startImage.src = '${theme}/gfx/play_inactive.png';
! 							stopImage.src = '${theme}/gfx/pause.png';
  							if(document.getElementById('mode').value != 'start'){
  								document.getElementById('mode').value = 'start';
  								getLog(lineCount);
  							}
  						}else if(action == 'stop'){
! 							startImage.src = '${theme}/gfx/play.png';
! 							stopImage.src = '${theme}/gfx/pause_inactive.png';
  							document.getElementById('mode').value = 'stop';
  						}
  					}
--- 99,113 ----
  						var startImage = document.getElementById('start');
  						var stopImage = document.getElementById('stop');
  						if(action == 'start'){
! 							startImage.src = '<%=request.getContextPath()%>/theme/${theme}/gfx/play_inactive.png';
! 							stopImage.src = '<%=request.getContextPath()%>/theme/${theme}/gfx/pause.png';
  							if(document.getElementById('mode').value != 'start'){
  								document.getElementById('mode').value = 'start';
  								getLog(lineCount);
  							}
  						}else if(action == 'stop'){
! 							startImage.src = '<%=request.getContextPath()%>/theme/${theme}/gfx/play.png';
! 							stopImage.src = '<%=request.getContextPath()%>/theme/${theme}/gfx/pause_inactive.png';
  							document.getElementById('mode').value = 'stop';
  						}
  					}
diff -crBE ./src/plugin/admin-system/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-system/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-system/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-system/src/webapp/WEB-INF/web.xml	2010-03-12 11:23:53.000000000 +0100
***************
*** 61,67 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 61,70 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/admin-url-upload/src/webapp/WEB-INF/web.xml ../local/src/plugin/admin-url-upload/src/webapp/WEB-INF/web.xml
*** ./src/plugin/admin-url-upload/src/webapp/WEB-INF/web.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/admin-url-upload/src/webapp/WEB-INF/web.xml	2010-03-12 11:23:43.000000000 +0100
***************
*** 55,61 ****
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>*</role-name>
  		</auth-constraint>
  	</security-constraint>
  
--- 55,64 ----
  			<url-pattern>/*</url-pattern>
  		</web-resource-collection>
  		<auth-constraint>
! 			<role-name>admin.portal</role-name>
!             <role-name>admin.portal.partner</role-name>
!             <role-name>admin.portal.partner.provider.index</role-name>
!             <role-name>admin.portal.partner.provider.catalog</role-name>
  		</auth-constraint>
  	</security-constraint>
  
diff -crBE ./src/plugin/build-plugin.xml ../local/src/plugin/build-plugin.xml
*** ./src/plugin/build-plugin.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/build-plugin.xml	2010-03-12 11:23:33.000000000 +0100
***************
*** 57,63 ****
      </fileset>
    	<fileset dir="${nutch.root}/lib/admin-ui-ext">
    		<include name="*.jar" />
!   	</fileset>  	
      <path refid="plugin.deps"/>
    </path>
  
--- 57,66 ----
      </fileset>
    	<fileset dir="${nutch.root}/lib/admin-ui-ext">
    		<include name="*.jar" />
!   	</fileset>
!   	<fileset dir="${nutch.root}/lib/ingrid-ext">
!   		<include name="*.jar" />
!   	</fileset>
      <path refid="plugin.deps"/>
    </path>
  
diff -crBE ./src/plugin/build.xml ../local/src/plugin/build.xml
*** ./src/plugin/build.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/build.xml	2010-03-12 11:23:36.000000000 +0100
***************
*** 94,99 ****
--- 94,102 ----
    	 <ant dir="admin-url-upload" target="deploy"/>
    	 <ant dir="admin-search" target="deploy"/>
    	 <ant dir="index-metadata" target="deploy"/>
+   	 <ant dir="admin-urlmaintenance" target="deploy"/>
+  	 <ant dir="admin-pd" target="deploy"/>
+  	 <ant dir="index-sns" target="deploy"/>
    </target>
  
    <!-- ====================================================== -->
***************
*** 207,211 ****
--- 210,217 ----
    	<ant dir="admin-url-upload" target="clean"/>
    	<ant dir="admin-search" target="clean"/>
    	<ant dir="index-metadata" target="clean"/>
+   	<ant dir="admin-urlmaintenance" target="clean"/>
+   	<ant dir="admin-pd" target="clean"/>
+   	<ant dir="index-sns" target="clean"/>
    </target>
  </project>
diff -crBE ./src/plugin/index-metadata/plugin.xml ../local/src/plugin/index-metadata/plugin.xml
*** ./src/plugin/index-metadata/plugin.xml	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/index-metadata/plugin.xml	2010-03-12 11:23:31.000000000 +0100
***************
*** 41,46 ****
--- 41,47 ----
          <parameter name="raw-fields" value="foo"/>
          <parameter name="fields" value="bar"/>
  -->        
+         <parameter name="raw-fields" value="datatype,funct_category,partner,topic,provider,lang"/>
        </implementation> 
                        
     </extension>
***************
*** 54,59 ****
--- 55,61 ----
          <parameter name="raw-fields" value="foo"/>
          <parameter name="fields" value="bar"/>
  -->        
+         <parameter name="raw-fields" value="datatype,funct_category,partner,topic,provider,lang"/>
        </implementation>
        
     </extension>
diff -crBE ./src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataQueryFilter.java ../local/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataQueryFilter.java
*** ./src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataQueryFilter.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataQueryFilter.java	2010-03-12 11:23:36.000000000 +0100
***************
*** 13,18 ****
--- 13,22 ----
  import org.apache.nutch.searcher.QueryException;
  import org.apache.nutch.searcher.QueryFilter;
  import org.apache.nutch.searcher.RawFieldQueryFilter;
+ import org.apache.lucene.index.Term;
+ import org.apache.lucene.search.TermQuery;
+ import org.apache.lucene.search.BooleanClause;
+ import org.apache.nutch.searcher.Query.Clause;
  
  public class MetadataQueryFilter implements QueryFilter {
  
***************
*** 45,50 ****
--- 49,58 ----
    private Configuration _conf;
  
    private List<QueryFilter> _queryFilterChain = new ArrayList<QueryFilter>();
+   
+   //------ none nutch-specific code starts here
+   private List<String> _rawFields = new ArrayList<String>();
+   //------ none nutch-specific code ends here
  
    @Override
    public BooleanQuery filter(Query input, BooleanQuery out)
***************
*** 52,59 ****
--- 60,100 ----
      for (QueryFilter queryFilter : _queryFilterChain) {
        queryFilter.filter(input, out);
      }
+     // ------ none nutch-specific code starts here
+     addNutchClauses(input.getNutchClauses(), out);
+     //------ none nutch-specific code ends here
      return out;
    }
+   //------ none nutch-specific code starts here
+   private boolean addNutchClauses(Query.Clause.NutchClause[] clauses, BooleanQuery query) {
+       boolean add = false;
+       for (Query.Clause.NutchClause clause : clauses) {
+           BooleanQuery newQuery = new BooleanQuery();
+           if (addNutchClause(clause, newQuery)) {
+               query.add(newQuery, getOccur(clause.isProhibited(), clause.isRequired()));
+               add = true;
+           }
+       }
+       return add;
+   }
+   
+   private boolean addNutchClause(Query.Clause.NutchClause clause, BooleanQuery query) {
+       boolean add = false;
+       for (Clause c : clause.getClauses()) {
+           String field = c.getField();
+           if (_rawFields.contains(field)) {
+               TermQuery term = new TermQuery(new Term(field, c.getTerm().toString()));
+               query.add(term, getOccur(c.isProhibited(), c.isRequired()));
+               add = true;
+           }
+       }
+       return add || addNutchClauses(clause.getNutchClauses(), query);
+   }
+   
+   private BooleanClause.Occur getOccur(boolean isProhibited, boolean isRequired) {
+       return isProhibited ? BooleanClause.Occur.MUST_NOT : isRequired ?  BooleanClause.Occur.MUST : BooleanClause.Occur.SHOULD;
+   }
+   //------ none nutch-specific code ends here
  
    @Override
    public Configuration getConf() {
***************
*** 74,79 ****
--- 115,123 ----
          if (rawFields != null) {
            String[] splits = rawFields.split(",");
            for (String split : splits) {
+             // ------ none nutch-specific code starts here
+             _rawFields.add(split);
+             //------ none nutch-specific code ends here
              _queryFilterChain.add(new UnTokenizedQueryFilter(split.trim()));
            }
          }
diff -crBE ./src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java ../local/src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java
*** ./src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java	2010-03-12 11:23:32.000000000 +0100
***************
*** 37,48 ****
--- 37,51 ----
  import org.apache.nutch.protocol.ProtocolOutput;
  import org.apache.nutch.protocol.ProtocolStatus;
  import org.apache.nutch.protocol.RobotRules;
+ import org.apache.nutch.tools.UrlReporter;
  import org.apache.nutch.util.GZIPUtils;
  import org.apache.nutch.util.DeflateUtils;
  import org.apache.nutch.util.LogUtil;
  
  // Hadoop imports
  import org.apache.hadoop.conf.Configuration;
+ import org.apache.hadoop.io.IntWritable;
+ import org.apache.hadoop.io.MapWritable;
  import org.apache.hadoop.io.Text;
  
  /**
***************
*** 223,228 ****
--- 226,236 ----
        }
        
        int code = response.getCode();
+       // ------ none nutch-specific code starts here
+       // add response code to metadata
+       MapWritable metaData = datum.getMetaData();
+       metaData.put(UrlReporter.RESPONSE_CODE, new IntWritable(code));
+       // ------ none nutch-specific code ends here
        byte[] content = response.getContent();
        Content c = new Content(u.toString(), u.toString(),
                                (content == null ? EMPTY_CONTENT : content),
diff -crBE ./src/test/org/apache/nutch/admin/GuiComponentExtensionContainerTest.java ../local/src/test/org/apache/nutch/admin/GuiComponentExtensionContainerTest.java
*** ./src/test/org/apache/nutch/admin/GuiComponentExtensionContainerTest.java	2010-01-26 02:47:32.000000000 +0100
--- ../local/src/test/org/apache/nutch/admin/GuiComponentExtensionContainerTest.java	2010-03-12 11:23:53.000000000 +0100
***************
*** 14,20 ****
  
    public void testGeneralGuiComponent() throws Exception {
      Configuration configuration = NutchConfiguration.create();
!     configuration.set("plugin.folders", "src/plugin");
      GuiComponentExtensionContainer guiComponentContainer = new GuiComponentExtensionContainer(
              configuration);
      List<Extension> list = guiComponentContainer
--- 14,20 ----
  
    public void testGeneralGuiComponent() throws Exception {
      Configuration configuration = NutchConfiguration.create();
!     configuration.set("plugin.folders", "101tec-nutch-11e55b9/src/plugin,portalu-nutch-gui/src/plugin");
      GuiComponentExtensionContainer guiComponentContainer = new GuiComponentExtensionContainer(
              configuration);
      List<Extension> list = guiComponentContainer
